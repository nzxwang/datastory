---
title: "Maximum Likelihood Estimate and Ordinary Least Squares"
author: "Nick Wang"
date: "January 30, 2019"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
```

##Maximum Likelihood Estimate
For bivariate data $(x_1,y_1),...,(x_n,y_n)$, the simple linear regression model says that $y_i$ is a random value generated by a random variable $$Y_i=ax_i+b+\epsilon_i$$ where $a, b, x_i$ are fixed values, and each $\epsilon_i$ is an independent and identically distributed normal random variable with mean $0$ and variance $\sigma^2$.

Since $\epsilon_i=Y_i-ax_i+b \sim N(0,\sigma^2)$ then $$\epsilon+(ax_i+b)=Y_i \sim N(ax_i+b,\sigma^2)$$ because a shifted normal random variable is still normal.

The probability distribution function for this normal random variable is $$f_{Y_i}(y_i)=(2\pi\sigma^2)^{(-\frac{1}{2})}e^{-\frac{(y_i-ax_i-b)^2}{2\sigma^2}}$$

Then the likelihood of our data is just the product of their individual likelihoods $$f(y_1,...y_n|\sigma,a,b,x_1,...,x_n)=(2\pi\sigma^2)^{(-\frac{n}{2})}e^{-\frac{\sum_{i=1}^{n}(y_i-ax_i-b)^2}{2\sigma^2}}$$

Maximizing the likelihood is difficult. Instead, take the logarithm (a monotonically increasing function) and maximize that instead $$\log [f(y_1,...y_n|\sigma,a,b,x_1,...,x_n)]=-\frac{n}{2}\log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-ax_i-b)^2$$

The first term is a constant, so the maximum likelihood estimate is found by maximizing the sum $$-\sum_{i=1}^{n}(y_i-ax_i-b)^2$$

##Least Squares
The model can also be formulated using matrices and vectors. Place the data $(x_1,y_1),...,(x_n,y_n)$, and errors $\epsilon_1,...,\epsilon_n$ into vectors $\mathbf{\vec{x}}$, $\mathbf{\vec{y}}$ and $\mathbf{\vec{\epsilon}}$ respectively. Place the coefficients $a,b$ into a vector $\mathbf{\vec{\beta}}$. Augment the $\mathbf{\vec{x}}$ vector with a vector of 1's to allow for the intercept term to get $\mathbf{\vec{y}} = \mathbf{X}\mathbf{\vec{\beta}}+\mathbf{\vec{\epsilon}}$ or
$$\left[\begin{array}
{c}
  y_1 \\
  \vdots \\
  y_n 
\end{array}\right] =
\left[\begin{array}
{c}
  1 & x_1 \\
  \vdots & \vdots \\
  1 & x_n 
\end{array}\right]
\left[\begin{array}
{c}
  b \\ a
\end{array}\right] -
\left[\begin{array}
{c}
  \epsilon_1 \\ \vdots \\ \epsilon_n
\end{array}\right]$$

Then the error is $\mathbf{\vec{\epsilon}}=\mathbf{\vec{y}} - \mathbf{X}\mathbf{\vec{\beta}}$. In minimizing this error vector by least squares, we end up with the same result as MLE

##Geometric Interpretation
Although this function can be analytically minimized by calculus, it also has geometric interpretation. Consider the following image uploaded to Wikipedia. 

```{r projection, echo=FALSE, out.width="50%"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/8/87/OLS_geometric_interpretation.svg")
```

We want to approximate the vector $\mathbf{\vec{y}}$ as closely as possible with a linear combination of the columns $\mathbf{X}$ (the regressors). The linear combinations $\mathbf{X}\mathbf{\vec{\beta}}$ span the column space of $\mathbf{X}$ represented by the green plane. The closest approximation $\mathbf{X}\mathbf{\hat{\beta}}$ is obtained when the error $\mathbf{\hat{\epsilon}}=\mathbf{\vec{y}} - \mathbf{X}\mathbf{\hat{\beta}}$ is orthogonal to the column space. Thus $\mathbf{X^T}(\mathbf{\vec{y}} - \mathbf{X}\mathbf{\vec{\beta}})=0$ which results in the famous equation $$\mathbf{X^T}\mathbf{X}\mathbf{\vec{\beta}}=\mathbf{X^T}\mathbf{\vec{y}}$$
where $\mathbf{\vec{\beta}}$ can be solved for directly, and $\mathbf{X}\mathbf{\hat{\beta}}$ is called the projection of $\mathbf{\vec{y}}$ onto the column space of $\mathbf{X}$.