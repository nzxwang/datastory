{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.callbacks.hooks import *\n",
    "from fastai.utils.mem import *\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = Path(\"/hpf/largeprojects/MICe/nwang/TissueVision/2019-05-31_Mallar_NeuralNet/processed\")\n",
    "images = sorted([raw_path for raw_path in processed_dir.ls() if \"_image\" in raw_path.as_posix()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = Path(\"/hpf/largeprojects/MICe/nwang/TissueVision/2019-05-31_Mallar_NeuralNet/models\")\n",
    "\n",
    "defaults.device = torch.device(\"cpu\")\n",
    "learn = load_learner(path = models_path, file = \"2019-06-12_RESNET50_IOU0.42.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageList' object has no attribute 'normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-0f7c6d646233>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m il = (ImageList.from_folder(processed_dir)\n\u001b[1;32m      2\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0mfilter_by_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'image'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m       \u001b[0;34m.\u001b[0m\u001b[0mfilter_by_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'1.2_Z0080_R_image_i6_j3.tif'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagenet_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageList' object has no attribute 'normalize'"
     ]
    }
   ],
   "source": [
    "il = (ImageList.from_folder(processed_dir)\n",
    "      .filter_by_func(lambda fname:'image' in Path(fname).name)\n",
    "      .filter_by_func(lambda fname:'1.2_Z0080_R_image_i6_j3.tif' in Path(fname).name)\n",
    "      .normalize(imagenet_stats)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageList (1 items)\n",
      "Image (3, 224, 224)\n",
      "Path: /hpf/largeprojects/MICe/nwang/TissueVision/2019-05-31_Mallar_NeuralNet/processed\n",
      "<class 'fastai.vision.image.Image'>\n",
      "Image (3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "print(il)\n",
    "print(type(il[0]))\n",
    "print(il[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = learn.predict(il[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageSegment (1, 224, 224)\n",
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]])\n",
      "tensor([[[0.6899, 0.7231, 0.7518,  ..., 0.8336, 0.8282, 0.8178],\n",
      "         [0.7059, 0.7345, 0.7779,  ..., 0.8194, 0.8094, 0.8162],\n",
      "         [0.8006, 0.8014, 0.8281,  ..., 0.8894, 0.8506, 0.8352],\n",
      "         ...,\n",
      "         [0.6832, 0.7308, 0.7709,  ..., 0.9131, 0.8887, 0.8203],\n",
      "         [0.6806, 0.7441, 0.7844,  ..., 0.9065, 0.8803, 0.8132],\n",
      "         [0.6992, 0.7400, 0.7960,  ..., 0.8870, 0.8474, 0.7853]],\n",
      "\n",
      "        [[0.3101, 0.2769, 0.2482,  ..., 0.1664, 0.1718, 0.1822],\n",
      "         [0.2941, 0.2655, 0.2221,  ..., 0.1806, 0.1906, 0.1838],\n",
      "         [0.1994, 0.1986, 0.1719,  ..., 0.1106, 0.1494, 0.1648],\n",
      "         ...,\n",
      "         [0.3168, 0.2692, 0.2291,  ..., 0.0869, 0.1113, 0.1797],\n",
      "         [0.3194, 0.2559, 0.2156,  ..., 0.0935, 0.1197, 0.1868],\n",
      "         [0.3008, 0.2600, 0.2040,  ..., 0.1130, 0.1526, 0.2147]]])\n"
     ]
    }
   ],
   "source": [
    "print(prediction[0])\n",
    "print(prediction[1])\n",
    "print(prediction[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAYAAAAaLWrhAAAABHNCSVQICAgIfAhkiAAAA0pJREFUeJzt3cFt4lAUQNHJKFVQBUWMRANpli6ogjYyixEJBJvA5NtXUc7ZAEYCb66e9b8FT3+eXl5/AYnf9QnATyZACAkQQgKEkAAhJEAICRBCAoSQACEkQAgJEEIChJAAISRACAkQQgKEkAAhJEAICRBCAoSQACEkQAgJEEIChJAAISRACAkQQgKEkAAhJEAICRBCAoSQACEkQAgJEEIChJAAISRACAkQQgKEkAAhJEAICRBCAoSQACEkQFjZ/nh4ey5AWMn+eHiL7/QoQIjsjwcBwhrOLztPdputAKEkQFjY1PQ7HRcghAQIIQFCSIAQEiAsbLfZzr4nQFjBXIQChJVMRfgcnAf8WOcR2geEmAAh4l5QiAkQQgKEkAAhJEAICRBCAoSQACEkQIY7//k9bhMgixHh5wQIIQFCSIAQEiCLufVTDPwjQBZjNfRzAmRxIpwnQIYS22MEyCqEOU2ArEaE1wTIMAJ7nAAhJEAICRBCAoSQACEkQIZx7+fjBAghATKUKfgYAbIacV4TIMNNhSa+af6gk0UI7j4mIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIoeclPnR/PFy83m22S3wNfHvDJ+DH+OaOAQMD3B8PN0MTIVwbEuC9cYkQLq2+CCNCeGcVFEJfDtBEg/9nAkLoy/uA53t890xDe4LwzgSE0NAAP0430w5uG34rmujgfqtegooTLi1yM/Y50cE8izAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAghAUJIgBASIIQECCEBQkiAEBIghAQIIQFCSIAQEiCEBAihv1csWEvaaFjMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "ImageSegment (1, 224, 224)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv.imread(images[0].as_posix(), cv.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = torch.tensor(image, dtype=torch.float32).unsqueeze(0)\n",
    "image3 = torch.cat((image1, image1, image1),0)\n",
    "image3.shape\n",
    "image_proper = Image(image3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = learn.predict(image_proper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
