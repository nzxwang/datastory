---
title: "Chebyshev's Inequality"
author: "Nick Wang"
date: "February 13, 2019"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
```
This Rmd takes heavily from _Dilip Sarwate_'s answer at https://math.stackexchange.com/questions/1344734/intuition-behind-chebyshevs-inequality

##Chebyshev's Inequality
The statement of Chebyshev's inequality is
$$\Pr(|X-\mu|\geq k\sigma) \leq \frac{1}{k^2}$$
That is, no more than $\frac{1}{k^2}$ of the distribution's values can be more than $k$ standard deviations from the mean, or equivalently, at least $1-\frac{1}{k^2}$ of the distribution's values are within $k$ standard deviations of the mean.

## Mechanical Intuition
The standard normal distribution can be represented by a histogram of its probability density function.
```{r pdf, echo=false}
mew <- 0
sd <- 1
tibble(x=seq(mew-4*sd, mew+4*sd)) %>% ggplot(aes(x)) + stat_function(fun=dnorm)
```

Let us discretize its probability density function by bins of size $dx=0.1$.
```{r pmf, echo=false}
dx<-0.1
df <- tibble(
  x = seq(mew-4*sd, mew+4*sd,dx),
  y = dnorm(x)
)

df %>% ggplot(aes(x=x,y=y)) + 
  geom_col()
```

```{r}
df %>% mutate(
  y2 = case_when(
    abs(x)<1 ~ dnorm(x),
    abs(x)==1 ~ pnorm(1),
    abs(x)>1 ~0
  )
) %>%
  ggplot(aes(x=x,y=y2)) + geom_col()
```


## General Intuition
The fact is, if $g(x) \geq h(x) ~\forall x \in \mathbb R$, then $E[g(X)] \geq E[h(X)]$ for any random variable $X$. Intuitively, since $g(X)$ is always at least as large as $h(X)$, the average value of $g(X)$ must be
at least as large as the average value of $h(X)$.

Consider the functions $$g(x) = (x-\mu)^2 ~ \text{and}~ 
h(x)= \begin{cases}a^2,& |x - \mu| \geq a,\\0, & |x-\mu|< a,\end{cases}$$
where $a > 0$. Visually, the intuitive fact becomes obvious.

```{r, echo=FALSE}
mew <- 10
mew_colour <- "grey"
a <- 5
a_colour <- "orange"
g <- function(x) (x-mew)^2
h <- function(x) {case_when(
  abs(x-mew)<a ~ 0,
  abs(x-mew)>=a ~ a^2
)}

dx <- 0.001
tibble(x = seq(mew-a*2.5,mew+a*2.5,by=dx),
       `g(x)` = g(x),
       `h(x)` = h(x)) %>%
  gather(key=func, value=y, -x) %>%
  ggplot(aes(x=x, y=y, colour=func)) +
  geom_line() +
  geom_vline(xintercept=mew, colour=mew_colour) +
  annotate("text", x=mew, y=-5, label=paste0("Î¼=",mew), colour=mew_colour, angle=0) +
  geom_segment(aes(x=mew, xend=mew+a, y=a^2/2, yend=a^2/2), colour=a_colour, arrow=arrow(length=unit(0.15, "inches"), ends="both")) +
  annotate("text", x=mew+a/2, y=a^2/2+7, label=paste0("a=",a), colour=a_colour)
```

Now just take the expectation of $g(x)$ and $h(x)$of some random variable $X$ with finite mean $\mu$ and finite variance $\sigma^2$.
$$E[g(X)] = E[(X-\mu)^2] := \sigma^2$$
$$E[h(X)] = \int_{\mu-a}^{\mu+a}a^2dx=a^2P\{|X-\mu|\geq a\} $$
Use the inequality from the intuitive fact $E[g(X)] \geq E[h(X)]$ to get 
$$\sigma^2 \geq a^2P\{|X-\mu|\geq a\}$$
The above equation is valid for any $a > 0$, so just set $a=k\sigma $, and one line of algebra to get Chebyshev's Inequality.

$$\sigma^2 \geq k^2\sigma^2P\{|X-\mu|\geq k\sigma\}$$
$$\Pr(|X-\mu|\geq k\sigma) \leq \frac{1}{k^2}$$