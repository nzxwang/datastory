---
title: "Chebyshev's Inequality"
author: "Nick Wang"
date: "February 13, 2019"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
```
This Rmd takes heavily from _Dilip Sarwate_'s answer at https://math.stackexchange.com/questions/1344734/intuition-behind-chebyshevs-inequality

##Chebyshev's Inequality
The statement of Chebyshev's inequality is
$$\Pr(|X-\mu|\geq k\sigma) \leq \frac{1}{k^2}$$
That is, no more than $\frac{1}{k^2}$ of the distribution's values can be more than $k$ standard deviations from the mean, or equivalently, at least $1-\frac{1}{k^2}$ of the distribution's values are within $k$ standard deviations of the mean.

## Mechanical Intuition
The standard normal distribution can be represented by a histogram of its probability density function.
```{r pdf, echo=FALSE}
mew <- 0
sd <- 1
tibble(x=seq(mew-4*sd, mew+4*sd)) %>% 
  ggplot(aes(x)) +
  stat_function(fun=dnorm)
```

Let us discretize its probability density function by bins of size $dx=0.1$, to get a probability mass function. Consider the variance $\sigma^2$ as representing the _moment of inertia_ of the probability mass about the center of mass $\mu=0$.
```{r pmf, echo=FALSE}
dx<-0.1
df <- tibble(
  x = seq(mew-4*sd, mew+4*sd,dx),
  y = dnorm(x)
)

df %>% ggplot(aes(x=x,y=y)) + 
  geom_col()
```

For $k=1.5$, imagine taking all the probability mass $M$ in the region $(-\infty, \mu-k\sigma] \cup [\mu+k\sigma, \infty)$ and moving it to $x=-k\sigma$ and $x=k\sigma$ for mass on the left and right of the mean respectively.
```{r pmf_changed, include=FALSE}
k=1.5
df %>% mutate(
  y2 = case_when(
    abs(x)<k ~ dnorm(x),
    abs(x)==k ~ pnorm(k),
    abs(x)>k ~0
  )
) %>%
  ggplot(aes(x=x,y=y2)) + geom_col()
```

This probability mass $M$ would then contribute $(k\sigma)^2M$ to the variance $\sigma^2$. This means that it originally contributed at least $(k\sigma)^2M$. Since $M$ is less than the total probability mass, the total variance must then be 
$$\sigma^2 \geq (k\sigma)^2M$$
By our definition of the probability mass $M$, it is simply the probability of mass being at least $k\sigma$ from $\mu$.
$$\sigma^2 \geq k^2\sigma^2P\{|X-\mu|\geq k\sigma\}$$
Then, divide through by $\sigma$, and QED.
$$\Pr(|X-\mu|\geq k\sigma) \leq \frac{1}{k^2}$$

Note that our choice of $k, \sigma, \mew$ and the standard normal distribution were all arbitrary. In the most extreme case, _equality_ holds when there are equal point masses of $\frac{1}{2k^2}$ at
$\mu \pm k\sigma$ and a point mass of $1 - \frac{1}{k^2}$ at
$\mu$. The central mass contributes nothing to the variance and the far away masses each contribute $\left(\frac{1}{2k^2}\right)(k\sigma)^2 = \frac{\sigma^2}{2}$.

## Mathematical Intuition
The fact is, if $g(x) \geq h(x) ~\forall x \in \mathbb R$, then $E[g(X)] \geq E[h(X)]$ for any random variable $X$. Intuitively, since $g(X)$ is always at least as large as $h(X)$, the average value of $g(X)$ must be
at least as large as the average value of $h(X)$.

Consider the functions $$g(x) = (x-\mu)^2 ~ \text{and}~ 
h(x)= \begin{cases}a^2,& |x - \mu| \geq a,\\0, & |x-\mu|< a,\end{cases}$$
where $a > 0$. Visually, the intuitive fact becomes obvious.

```{r, echo=FALSE}
mew <- 10
mew_colour <- "grey"
a <- 5
a_colour <- "orange"
g <- function(x) (x-mew)^2
h <- function(x) {case_when(
  abs(x-mew)<a ~ 0,
  abs(x-mew)>=a ~ a^2
)}

dx <- 0.001
tibble(x = seq(mew-a*2.5,mew+a*2.5,by=dx),
       `g(x)` = g(x),
       `h(x)` = h(x)) %>%
  gather(key=func, value=y, -x) %>%
  ggplot(aes(x=x, y=y, colour=func)) +
  geom_line() +
  geom_vline(xintercept=mew, colour=mew_colour) +
  annotate("text", x=mew, y=-5, label=paste0("Î¼=",mew), colour=mew_colour, angle=0) +
  geom_segment(aes(x=mew, xend=mew+a, y=a^2/2, yend=a^2/2), colour=a_colour, arrow=arrow(length=unit(0.15, "inches"), ends="both")) +
  annotate("text", x=mew+a/2, y=a^2/2+7, label=paste0("a=",a), colour=a_colour)
```

Now just take the expectation of $g(x)$ and $h(x)$of some random variable $X$ with finite mean $\mu$ and finite variance $\sigma^2$.
$$E[g(X)] = E[(X-\mu)^2] := \sigma^2$$
$$E[h(X)] = \int_{\mu-a}^{\mu+a}a^2dx=a^2P\{|X-\mu|\geq a\} $$
Use the inequality from the intuitive fact $E[g(X)] \geq E[h(X)]$ to get 
$$\sigma^2 \geq a^2P\{|X-\mu|\geq a\}$$
The above equation is valid for any $a \gt 0$, so just set $a=k\sigma $, and one line of algebra to get Chebyshev's Inequality.
$$\sigma^2 \geq k^2\sigma^2P\{|X-\mu|\geq k\sigma\}$$
$$\Pr(|X-\mu|\geq k\sigma) \leq \frac{1}{k^2}$$